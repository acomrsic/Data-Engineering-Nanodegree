{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The data sources will be aggregated using Spark SQL and matplotlib will be used to display graphs of the data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from us_state_abbrev import state_udf, abbrev_state, abbrev_state_udf,city_code_udf,city_codes\n",
    "from immigration_codes import country_udf\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build SQL context object\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project will pull data from all sources and create fact and dimension tables to show movement of immigration.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "1. **U.S. City Demographic Data:** comes from [OpenSoft](https://public.opendatasoft.com) and includes data by city, state, age, population, veteran status and race.\n",
    "2. **I94 Immigration Data:** comes from the [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "and includes details on incoming immigrants and their ports of entry.\n",
    "3. **Airport Code Table:** comes from [datahub.io](https://datahub.io/core/airport-codes#data) and includes airport codes and corresponding cities.\n",
    "4. **World Temperature Data:** comes from [kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) and includes data on temperature changes in the U.S. since 1850."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "demog=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "airport=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "temperatureData=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByState.csv\")\n",
    "df_spark=spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "1. Filter average temperature data only for the United States and only year == 2013 and \n",
    "    create new fields with year, month, fahrenheit and run abbreviations function and drop duplicates.\n",
    "2. Remove nulls then convert i94res codes to country of origin then select important columns from the immigration data and drop duplicates.\n",
    "3. Sort city demographic data then calculate percentages and select percentages fields and drop duplicates.\n",
    "4. Filter airport data for \"small_airport\" and use substring to return the state code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Data by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the world Temperature Data for only the U.S. and only == 2013 and drop duplicates and convert celcius temp to f\n",
    "usTemperatures=temperatureData.filter(temperatureData[\"country\"]==\"United States\")\\\n",
    ".filter(year(temperatureData[\"dt\"])==2013)\\\n",
    ".withColumn(\"year\",year(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"month\",month(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    ".withColumn(\"state_abbrev\",state_udf(temperatureData[\"State\"]))\n",
    "\n",
    "new_Temperatures=usTemperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                       round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       \"state_abbrev\",\"State\",\"Country\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|state_abbrev|        State|      Country|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|2013|    7|            23.4|               74.1|          MA|Massachusetts|United States|\n",
      "|2013|    3|            -1.9|               28.5|          SD| South Dakota|United States|\n",
      "|2013|    9|            14.1|               57.4|          ME|        Maine|United States|\n",
      "|2013|    1|            -1.3|               29.7|          PA| Pennsylvania|United States|\n",
      "|2013|    9|            25.1|               77.2|          AL|      Alabama|United States|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_Temperatures.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration Data by State with Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nulls then convert i94res codes to country of origin and filter out NULLS and run country_udf function to show state names\n",
    "# country_udf, abbrev_state_udf and city_code_udf were created with data from i94 SAS labels Descriptions file.\n",
    "i94_data=df_spark.filter(df_spark.i94addr.isNotNull())\\\n",
    ".filter(df_spark.i94res.isNotNull())\\\n",
    ".filter(col(\"i94addr\").isin(list(abbrev_state.keys())))\\\n",
    ".filter(col(\"i94port\").isin(list(city_codes.keys())))\\\n",
    ".withColumn(\"origin_country\",country_udf(df_spark[\"i94res\"]))\\\n",
    ".withColumn(\"dest_state_name\",abbrev_state_udf(df_spark[\"i94addr\"]))\\\n",
    ".withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    ".withColumn(\"city_port_name\",city_code_udf(df_spark[\"i94port\"]))\n",
    "\n",
    "new_I94_Data=i94_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                             \"origin_country\",\"i94port\",\"city_port_name\",col(\"i94addr\").alias(\"state_code\"),\"dest_state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "|cicid|year|month|origin_country|i94port|    city_port_name|state_code|dest_state_name|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "| 41.0|2016|    6|   SOUTH KOREA|    SFR|SAN FRANCISCO     |        CA|     California|\n",
      "| 42.0|2016|    6|   SOUTH KOREA|    SFR|SAN FRANCISCO     |        CA|     California|\n",
      "| 45.0|2016|    6|       ROMANIA|    HOU|HOUSTON           |        TX|          Texas|\n",
      "| 52.0|2016|    6|       ALBANIA|    BOS|BOSTON            |        MA|  Massachusetts|\n",
      "| 53.0|2016|    6|       ALBANIA|    NEW|NEWARK/TETERBORO  |        PA|   Pennsylvania|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_I94_Data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Demographic Data by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate percentages of each numeric column and create new columns.\n",
    "demog_data=demog\\\n",
    ".withColumn(\"Median Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    ".withColumn(\"pct_male_pop\",demog[\"Male Population\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_female_pop\",demog[\"Female Population\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_veterans\",demog[\"Number of Veterans\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_foreign_born\",demog[\"Foreign-born\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_race\",demog[\"Count\"]/demog[\"Total Population\"]*100)\\\n",
    ".orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns with new calculated percentages and state names.\n",
    "new_demog_data=demog_data.select(\"State\",col(\"State Code\").alias(\"state_code\"),\\\n",
    "                                 col(\"Median Age\").alias(\"median_age\"),\\\n",
    "                                 \"pct_male_pop\",\\\n",
    "                                 \"pct_female_pop\",\\\n",
    "                                 \"pct_veterans\",\\\n",
    "                                 \"pct_foreign_born\",\\\n",
    "                                 \"Race\",\\\n",
    "                                 \"pct_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot the Race column\n",
    "pivot_demog_data=new_demog_data.groupBy(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\\\n",
    "                                    \"pct_female_pop\",\"pct_veterans\",\\\n",
    "                                    \"pct_foreign_born\").pivot(\"Race\").avg(\"pct_race\")\n",
    "\n",
    "#change the header name of the race fields for spark compatibility.\n",
    "pivot_demog_data=pivot_demog_data.select(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\"pct_veterans\",\"pct_foreign_born\",\\\n",
    "                                         col(\"American Indian and Alaska Native\").alias(\"native_american\"),\\\n",
    "                                         col(\"Asian\"),col(\"Black or African-American\").alias(\"Black\"),\\\n",
    "                                         col(\"Hispanic or Latino\").alias(\"hispanic_or_latino\"),\"White\")\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the average of each column per state. The avg function will distort the column names but will fix next step.\n",
    "pivot=pivot_demog_data.groupBy(\"State\",\"state_code\").avg(\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\\\n",
    "                                                       \"pct_veterans\",\"pct_foreign_born\",\"native_american\",\\\n",
    "                                                       \"Asian\",\"Black\",\"hispanic_or_latino\",\"White\").orderBy(\"State\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round the percentages and fix column names\n",
    "pivot=pivot.select(\"State\",\"state_code\",round(col(\"avg(median_age)\"),1).alias(\"median_age\"),\\\n",
    "                  round(col(\"avg(pct_male_pop)\"),1).alias(\"pct_male_pop\"),\\\n",
    "                   round(col(\"avg(pct_female_pop)\"),1).alias(\"pct_female_pop\"),\\\n",
    "                   round(col(\"avg(pct_veterans)\"),1).alias(\"pct_veterans\"),\\\n",
    "                   round(col(\"avg(pct_foreign_born)\"),1).alias(\"pct_foreign_born\"),\\\n",
    "                   round(col(\"avg(native_american)\"),1).alias(\"native_american\"),\\\n",
    "                   round(col(\"avg(Asian)\"),1).alias(\"Asian\"),\\\n",
    "                   round(col(\"avg(hispanic_or_latino)\"),1).alias(\"hispanic_or_latino\"),\\\n",
    "                   round(col(\"avg(Black)\"),1).alias(\"Black\"),\\\n",
    "                   round(col('avg(White)'),1).alias('White')\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|     State|state_code|median_age|pct_male_pop|pct_female_pop|pct_veterans|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+----------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|   Alabama|        AL|      36.2|        47.2|          52.8|         6.8|             5.1|            0.8|  2.9|               3.6| 45.0| 52.0|\n",
      "|    Alaska|        AK|      32.2|        51.2|          48.8|         9.2|            11.1|           12.2| 12.3|               9.1|  7.7| 71.2|\n",
      "|   Arizona|        AZ|      35.0|        48.8|          51.2|         6.6|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|  Arkansas|        AR|      32.8|        48.4|          51.6|         5.2|            10.7|            1.8|  4.1|              14.2| 21.8| 68.0|\n",
      "|California|        CA|      36.2|        49.4|          50.6|         4.1|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "+----------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Airport Data by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter airport data for 'small_airport' in the U.S. and use substring to show state\n",
    "airport_data=airport.filter(airport[\"type\"]==\"small_airport\")\\\n",
    ".filter(airport[\"iso_country\"]==\"US\")\\\n",
    ".withColumn(\"iso_region\",substring(airport[\"iso_region\"],4,2))\\\n",
    ".withColumn(\"elevation_ft\",col(\"elevation_ft\").cast(\"float\"))\n",
    "\n",
    "#Find average elevation per state\n",
    "airport_data_elevation=airport_data.groupBy(\"iso_country\",\"iso_region\").avg(\"elevation_ft\")\n",
    "\n",
    "#Select relevant columns and drop duplicates\n",
    "new_airport_data=airport_data_elevation.select(col(\"iso_country\").alias(\"country\"),\\\n",
    "                                               col(\"iso_region\").alias(\"state\"),\\\n",
    "                                               round(col(\"avg(elevation_ft)\"),1).alias(\"avg_elevation_ft\")).orderBy(\"iso_region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+\n",
      "|country|state|avg_elevation_ft|\n",
      "+-------+-----+----------------+\n",
      "|     US|   AK|           545.1|\n",
      "|     US|   AL|           414.6|\n",
      "|     US|   AR|           488.4|\n",
      "|     US|   AZ|          3098.0|\n",
      "|     US|   CA|          1261.4|\n",
      "+-------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_airport_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "##### Star Schema\n",
    "1. Dimension Tables\n",
    "    * airport_table\n",
    "        * country, state, avg_elevation_ft\n",
    "    * city_stats\n",
    "        * State,state_code,median_age,pct_male_pop,pct_female_pop,pct_veterans,pct_foreign_born,native_american,Asian,hispanic_or_latino,Black,White\n",
    "    * immigration_table\n",
    "        * cicid,year,month,origin_country,i94port,city_port_us,state,dest_state_us\n",
    "    * avg_state_temps\n",
    "        * year,month,AverageTemperature,avg_temp_fahrenheit,state_abbrev,State,Country\n",
    "2. Fact Table\n",
    "    * immigration_fact_table\n",
    "        * year, immig_month, immig_origin, immig_state, 'to_immig_state_count', 'avg_temp_fahrenheit', 'avg_elevation_ft',\n",
    "          'pct_foreign_born', 'native_american', 'Asian', 'hispanic_or_latino', 'Black', 'White'\n",
    "\n",
    "This schema was chosen because of it's simplicity and it's use in data analytics.\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1. Dimension tables will be created from cleansed data.\n",
    "2. Fact table is created as a SQL query with joins to dimension tables.\n",
    "3. Fact table is converted back to a spark dataframe.\n",
    "4. Fact table is written as final parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dimension tables\n",
    "new_I94_Data.createOrReplaceTempView(\"immigration\")\n",
    "pivot.createOrReplaceTempView(\"demographics\")\n",
    "new_airport_data.createOrReplaceTempView(\"airport\")\n",
    "new_Temperatures.createOrReplaceTempView(\"temperature\")\n",
    "\n",
    "#allow unlimited time for SQL joins and parquet writes.\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will build the fact table by joining to the dimension tables above.\n",
    "# We are counting how many people immigrated to each state in the U.S.\n",
    "immigration_to_states=spark.sql(\"\"\"SELECT \n",
    "                                    m.year,\n",
    "                                    m.month AS immig_month,\n",
    "                                    m.origin_country AS immig_origin,\n",
    "                                    m.dest_state_name AS to_immig_state,\n",
    "                                    COUNT(m.state_code) AS to_immig_state_count,\n",
    "                                    t.avg_temp_fahrenheit,\n",
    "                                    a.avg_elevation_ft,\n",
    "                                    d.pct_foreign_born,\n",
    "                                    d.native_american,\n",
    "                                    d.Asian,\n",
    "                                    d.hispanic_or_latino,\n",
    "                                    d.Black,\n",
    "                                    d.White\n",
    "                                    \n",
    "                                    FROM immigration m JOIN temperature t ON m.state_code=t.state_abbrev AND m.month=t.month\n",
    "                                    JOIN demographics d ON d.state_code=t.state_abbrev\n",
    "                                    JOIN airport a ON a.state=t.state_abbrev\n",
    "                                    \n",
    "                                    GROUP BY m.year,m.month, m.origin_country,\\\n",
    "                                    m.dest_state_name,m.state_code,t.avg_temp_fahrenheit,a.avg_elevation_ft,\\\n",
    "                                    d.pct_foreign_born,d.native_american,\\\n",
    "                                    d.Asian,d.hispanic_or_latino,\\\n",
    "                                    d.hispanic_or_latino,d.White,\\\n",
    "                                    d.Black\n",
    "                                    \n",
    "                                    ORDER BY m.origin_country,m.state_code\n",
    "                                    \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|year|immig_month|immig_origin|to_immig_state|to_immig_state_count|avg_temp_fahrenheit|avg_elevation_ft|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|2016|          6| AFGHANISTAN|      Arkansas|                   1|               77.7|           488.4|            10.7|            1.8|  4.1|              14.2| 21.8| 68.0|\n",
      "|2016|          6| AFGHANISTAN|       Arizona|                   1|               79.9|          3098.0|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|2016|          6| AFGHANISTAN|    California|                  31|               72.5|          1261.4|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "|2016|          6| AFGHANISTAN|      Colorado|                   1|               65.9|          5912.8|             9.6|            2.0|  4.9|              22.2|  4.2| 88.0|\n",
      "|2016|          6| AFGHANISTAN|   Connecticut|                   4|               67.3|           490.3|            25.2|            1.3|  5.3|              34.8| 24.3| 59.6|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_to_states.toDF('year', 'immig_month', 'immig_origin', 'to_immig_state', \\\n",
    "          'to_immig_state_count', 'avg_temp_fahrenheit', 'avg_elevation_ft',\\\n",
    "          'pct_foreign_born', 'native_american', 'Asian', 'hispanic_or_latino', 'Black', 'White').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write fact table to parquet\n",
    "immigration_to_states.write.parquet(\"immigration_to_states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+\n",
      "| year|month|country|state|\n",
      "+-----+-----+-------+-----+\n",
      "|false|false|  false|false|\n",
      "+-----+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for NULL values in year, month, origin_country, to_immig_state\n",
    "# If false is retured for all columns selected then the data is fine across the dataset\n",
    "immigration_to_states.select(isnull('year').alias('year'),\\\n",
    "                             isnull('immig_month').alias('month'),\\\n",
    "                             isnull('immig_origin').alias('country'),\\\n",
    "                             isnull('to_immig_state').alias('state')).dropDuplicates().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|fact_table_count|\n",
      "+----------------+\n",
      "|         3207230|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of people immigrated to the United States from the Fact Table.\n",
    "immigration_to_states.select(sum('to_immig_state_count').alias('fact_table_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3214208|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of immigrants from the source table new_I94_Data\n",
    "spark.sql('SELECT COUNT(*) FROM immigration').show()\n",
    "#new_I94_Data.select(sum('origin_country').alias('i94Count')).show()\n",
    "# Both totals must match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "* See the DataDictionary file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. For this project I used Apache Spark to read, transform, and create data outputs for further analysis. The reason for this was due to the small amount of data and the speed of Spark.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The data should be updated quarterly. This gives the most up-to-date data for government and organizations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Under the following scenarios, I would approach the problem differently:**\n",
    "  * **If the data was increased by 100x, I would use Apache Hadoop to create a distributed processing system for faster processing.**\n",
    "  * **To update on a daily basis I would use Apache Airflow to create a schedule to run a distributed update on all tables with data streamed from the source.**\n",
    "  * **If the data needs to be accessed by 100+ people, I would use a web app running on Amazon AWS for increased capacity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
